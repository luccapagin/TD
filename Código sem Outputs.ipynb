{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9d56cc",
   "metadata": {},
   "source": [
    "# Atualização do tccfinal.ipynb com correções e melhores práticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbcded",
   "metadata": {},
   "source": [
    "## 0. Instalação de Bibliotecas (Se necessário)\n",
    "\n",
    "Certifique-se de que o TensorFlow está instalado no seu ambiente. Se não estiver, execute a célula abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow matplotlib pandas numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fadb73",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b76b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, regularizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84260a5e",
   "metadata": {},
   "source": [
    "## 2. Hiperparâmetros e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d435c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 3      # Número de execuções para significância estatística\n",
    "EPOCHS = 50       # Número original de épocas\n",
    "PATIENCE = 5      # Paciência para Early Stopping\n",
    "BATCH_SIZE = 32   # Tamanho do lote\n",
    "L2_FACTOR = 0.001 # Fator de regularização L2\n",
    "DROPOUT_RATE = 0.5 # Taxa de Dropout\n",
    "LEARNING_RATE = 0.001 # Taxa de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2effbd3",
   "metadata": {},
   "source": [
    "## 3. Carregamento e Pré-processamento do Dataset CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd52d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_full, y_train_full), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalizar valores dos pixels para o intervalo [0, 1]\n",
    "x_train_full, x_test = x_train_full / 255.0, x_test / 255.0\n",
    "\n",
    "# Dividir dados de treinamento completos em treino e validação\n",
    "val_split = 10000 # Usar 10000 amostras para validação\n",
    "x_train, x_val = x_train_full[val_split:], x_train_full[:val_split]\n",
    "y_train, y_val = y_train_full[val_split:], y_train_full[:val_split]\n",
    "\n",
    "print(f\"Amostras de Treinamento: {len(x_train)}\")\n",
    "print(f\"Amostras de Validação: {len(x_val)}\")\n",
    "print(f\"Amostras de Teste: {len(x_test)}\")\n",
    "\n",
    "# Nomes das classes para plotagem\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440af2c9",
   "metadata": {},
   "source": [
    "## 4. Funções de Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff6c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title):\n",
    "    \"\"\"Plota curvas de acurácia e perda para um único histórico de treinamento.\"\"\"\n",
    "    acc = history.history.get('accuracy')\n",
    "    val_acc = history.history.get('val_accuracy')\n",
    "    loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "\n",
    "    has_acc = acc is not None and val_acc is not None\n",
    "    has_loss = loss is not None and val_loss is not None\n",
    "\n",
    "    if not has_acc and not has_loss:\n",
    "        print(f\"Aviso: Histórico vazio ou incompleto para '{title}'. Não é possível plotar.\")\n",
    "        return\n",
    "\n",
    "    epochs_with_data = 0\n",
    "    if has_acc:\n",
    "        epochs_with_data = len(acc)\n",
    "    elif has_loss:\n",
    "        epochs_with_data = len(loss)\n",
    "\n",
    "    if epochs_with_data == 0:\n",
    "        print(f\"Aviso: Histórico sem épocas registradas para '{title}'. Não é possível plotar.\")\n",
    "        return\n",
    "\n",
    "    epochs_range = range(1, epochs_with_data + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    if has_acc:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc, 'o-', label='Acurácia Treinamento', markersize=4)\n",
    "        plt.plot(epochs_range, val_acc, '-', label='Acurácia Validação')\n",
    "        plt.title(f'Acurácia - {title}')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Acurácia')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'Acurácia - {title}')\n",
    "        plt.text(0.5, 0.5, 'Dados de Acurácia Indisponíveis', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Acurácia')\n",
    "\n",
    "    if has_loss:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss, 'o-', label='Perda Treinamento', markersize=4)\n",
    "        plt.plot(epochs_range, val_loss, '-', label='Perda Validação')\n",
    "        plt.title(f'Perda - {title}')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Perda')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Perda - {title}')\n",
    "        plt.text(0.5, 0.5, 'Dados de Perda Indisponíveis', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Perda')\n",
    "\n",
    "    plt.suptitle(f'Desempenho do Modelo: {title}', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_overfitting_gap(history, title):\n",
    "    \"\"\"Plota a diferença entre as métricas de treino e validação.\"\"\"\n",
    "    acc = history.history.get('accuracy')\n",
    "    val_acc = history.history.get('val_accuracy')\n",
    "    loss = history.history.get('loss')\n",
    "    val_loss = history.history.get('val_loss')\n",
    "\n",
    "    has_acc = acc is not None and val_acc is not None\n",
    "    has_loss = loss is not None and val_loss is not None\n",
    "\n",
    "    if not has_acc and not has_loss:\n",
    "        return\n",
    "\n",
    "    epochs_with_data = 0\n",
    "    if has_acc:\n",
    "        epochs_with_data = min(len(acc), len(val_acc))\n",
    "    elif has_loss:\n",
    "        epochs_with_data = min(len(loss), len(val_loss))\n",
    "\n",
    "    if epochs_with_data == 0:\n",
    "        return\n",
    "\n",
    "    epochs_range = range(1, epochs_with_data + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    if has_acc:\n",
    "        acc_gap = [t - v for t, v in zip(acc[:epochs_with_data], val_acc[:epochs_with_data])]\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc_gap, '-', label='Gap Acurácia (Treino - Val)')\n",
    "        plt.title(f'Gap de Acurácia - {title}')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Acurácia Treino - Acurácia Validação')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'Gap de Acurácia - {title}')\n",
    "        plt.text(0.5, 0.5, 'Dados Indisponíveis', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Acurácia Treino - Acurácia Validação')\n",
    "\n",
    "    if has_loss:\n",
    "        loss_gap = [v - t for t, v in zip(loss[:epochs_with_data], val_loss[:epochs_with_data])]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss_gap, '-', label='Gap Perda (Val - Treino)')\n",
    "        plt.title(f'Gap de Perda - {title}')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Perda Validação - Perda Treino')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Gap de Perda - {title}')\n",
    "        plt.text(0.5, 0.5, 'Dados Indisponíveis', horizontalalignment='center', verticalalignment='center')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel('Perda Validação - Perda Treino')\n",
    "\n",
    "    plt.suptitle(f'Análise de Overfitting (Gap Treino-Validação): {title}', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def compare_histories(histories_dict, title=\"Comparação de Acurácia de Validação (Última Execução)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_successful = False\n",
    "    for label, history in histories_dict.items():\n",
    "        if history is not None and 'val_accuracy' in history.history and history.history['val_accuracy']:\n",
    "            epochs_range = range(1, len(history.history['val_accuracy']) + 1)\n",
    "            plt.plot(epochs_range, history.history['val_accuracy'], label=label)\n",
    "            plot_successful = True\n",
    "        else:\n",
    "            print(f\"Aviso: Histórico ou 'val_accuracy' não encontrado/vazio para '{label}'.\")\n",
    "\n",
    "    if not plot_successful:\n",
    "        print(\"Nenhum dado de acurácia de validação para plotar na comparação.\")\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia de Validação')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "\n",
    "def compare_validation_loss(histories_dict, title=\"Comparação de Perda de Validação (Última Execução)\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_successful = False\n",
    "    for label, history in histories_dict.items():\n",
    "        if history is not None and 'val_loss' in history.history and history.history['val_loss']:\n",
    "            epochs_range = range(1, len(history.history['val_loss']) + 1)\n",
    "            plt.plot(epochs_range, history.history['val_loss'], label=label)\n",
    "            plot_successful = True\n",
    "        else:\n",
    "            print(f\"Aviso: Histórico ou 'val_loss' não encontrado/vazio para '{label}'.\")\n",
    "\n",
    "    if not plot_successful:\n",
    "        print(\"Nenhum dado de perda de validação para plotar na comparação.\")\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda de Validação')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5be779",
   "metadata": {},
   "source": [
    "## 5. Definições dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd1a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitetura CNN Baseline (Base)\n",
    "def create_model_baseline():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"Baseline\")\n",
    "    return model\n",
    "\n",
    "# CNN com Regularização L1\n",
    "def create_model_l1():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(L2_FACTOR), input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(L2_FACTOR)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(L2_FACTOR)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(L2_FACTOR)),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"L1_Regularization\")\n",
    "    return model\n",
    "\n",
    "# CNN com Regularização L2\n",
    "def create_model_l2():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR), input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_FACTOR)),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"L2_Regularization\")\n",
    "    return model\n",
    "\n",
    "# CNN com Regularização L1+L2 (Elastic Net)\n",
    "def create_model_l1_l2():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=L2_FACTOR, l2=L2_FACTOR), input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=L2_FACTOR, l2=L2_FACTOR)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1_l2(l1=L2_FACTOR, l2=L2_FACTOR)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=L2_FACTOR, l2=L2_FACTOR)),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"L1_L2_Regularization\")\n",
    "    return model\n",
    "\n",
    "# CNN com Dropout\n",
    "def create_model_dropout():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(DROPOUT_RATE),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"Dropout\")\n",
    "    return model\n",
    "\n",
    "# CNN com Batch Normalization\n",
    "def create_model_batch_norm():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), padding='same', use_bias=False, input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(10)\n",
    "    ], name=\"Batch_Normalization\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbbd42",
   "metadata": {},
   "source": [
    "## 6. Configuração do Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d9f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9bf3d",
   "metadata": {},
   "source": [
    "## 7. Loop de Treinamento para Múltiplas Execuções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"Baseline\": create_model_baseline,\n",
    "    \"L1\": create_model_l1,\n",
    "    \"L2\": create_model_l2,\n",
    "    \"L1_L2\": create_model_l1_l2,\n",
    "    \"Dropout\": create_model_dropout,\n",
    "    \"EarlyStopping\": create_model_baseline,\n",
    "    \"BatchNorm\": create_model_batch_norm,\n",
    "    \"DataAugmentation\": create_model_baseline\n",
    "}\n",
    "\n",
    "results = {name: [] for name in model_configs.keys()}\n",
    "histories_last_run = {name: None for name in model_configs.keys()}\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"\\n--- Iniciando Execução {run + 1}/{NUM_RUNS} ---\")\n",
    "    seed = run\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for name, model_creator in model_configs.items():\n",
    "        print(f\"--- Treinando modelo: {name} (Run {run + 1}) ---\")\n",
    "        model = model_creator()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        run_callbacks = []\n",
    "        if name == \"EarlyStopping\":\n",
    "            early_stopping_callback = callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=PATIENCE,\n",
    "                verbose=1,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            run_callbacks.append(early_stopping_callback)\n",
    "\n",
    "        if name == \"DataAugmentation\":\n",
    "            history = model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "                                steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                verbose=1,\n",
    "                                callbacks=run_callbacks)\n",
    "        else:\n",
    "            history = model.fit(x_train, y_train,\n",
    "                                epochs=EPOCHS,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                verbose=1,\n",
    "                                callbacks=run_callbacks)\n",
    "\n",
    "        val_loss = history.history.get('val_loss', [])\n",
    "        val_acc = history.history.get('val_accuracy', [])\n",
    "\n",
    "        if not val_acc or not val_loss:\n",
    "            print(f\"Aviso: Sem dados de validação (acc/loss) para {name} na execução {run + 1}. Pulando registro de resultados.\")\n",
    "            best_val_acc = 0\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_acc_epoch_index = -1\n",
    "            final_epoch = len(history.epoch)\n",
    "        else:\n",
    "            best_val_acc_epoch_index = np.argmax(val_acc)\n",
    "            best_val_acc = val_acc[best_val_acc_epoch_index]\n",
    "            best_val_loss = val_loss[best_val_acc_epoch_index]\n",
    "            final_epoch = len(history.epoch)\n",
    "\n",
    "        results[name].append({\n",
    "            'run': run + 1,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'val_loss_at_best_acc': best_val_loss,\n",
    "            'best_epoch': best_val_acc_epoch_index + 1 if best_val_acc_epoch_index != -1 else 0,\n",
    "            'total_epochs_ran': final_epoch\n",
    "        })\n",
    "\n",
    "        if run == NUM_RUNS - 1:\n",
    "            histories_last_run[name] = history\n",
    "            if val_acc and val_loss:\n",
    "                plot_title = f\"{name} (Execução Final: {run + 1})\"\n",
    "                plot_history(history, plot_title)\n",
    "                plot_overfitting_gap(history, plot_title)\n",
    "            else:\n",
    "                print(f\"Não foi possível plotar o histórico individual para {name} na execução final (dados ausentes).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dbc4d",
   "metadata": {},
   "source": [
    "## 8. Análise Quantitativa dos Resultados (Conjunto de Validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Resultados Quantitativos (Conjunto de Validação) ---\")\n",
    "summary_data = []\n",
    "for name, run_results in results.items():\n",
    "    if not run_results:\n",
    "        print(f\"Sem resultados registrados para {name}\")\n",
    "        summary_data.append({\n",
    "            \"Técnica\": name,\n",
    "            f\"Val Acc Média (±std)\": \"N/A\",\n",
    "            f\"Melhor Época Média (±std)\": \"N/A\",\n",
    "            f\"Épocas Totais Média (±std)\": \"N/A\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    best_val_accs = [r['best_val_acc'] for r in run_results]\n",
    "    best_epochs = [r['best_epoch'] for r in run_results if r['best_epoch'] > 0]\n",
    "    total_epochs = [r['total_epochs_ran'] for r in run_results]\n",
    "\n",
    "    mean_best_val_acc = np.mean(best_val_accs)\n",
    "    std_best_val_acc = np.std(best_val_accs)\n",
    "\n",
    "    mean_best_epoch = np.mean(best_epochs) if best_epochs else 0\n",
    "    std_best_epoch = np.std(best_epochs) if best_epochs else 0\n",
    "\n",
    "    mean_total_epochs = np.mean(total_epochs)\n",
    "    std_total_epochs = np.std(total_epochs)\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Técnica\": name,\n",
    "        f\"Val Acc Média (±std)\": f\"{mean_best_val_acc:.4f} ± {std_best_val_acc:.4f}\",\n",
    "        f\"Melhor Época Média (±std)\": f\"{mean_best_epoch:.1f} ± {std_best_epoch:.1f}\" if best_epochs else \"N/A\",\n",
    "        f\"Épocas Totais Média (±std)\": f\"{mean_total_epochs:.1f} ± {std_total_epochs:.1f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352ac6d",
   "metadata": {},
   "source": [
    "## 9. Comparação Visual (Apenas da Última Execução)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Comparação Visual (Última Execução) ---\")\n",
    "if any(hist is not None for hist in histories_last_run.values()):\n",
    "    compare_histories(histories_last_run)\n",
    "    compare_validation_loss(histories_last_run)\n",
    "else:\n",
    "    print(\"Nenhum histórico da última execução disponível para comparação visual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159cdba",
   "metadata": {},
   "source": [
    "## 10. Avaliação Final no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Avaliação Final no Conjunto de Teste ---\")\n",
    "\n",
    "acc_column_name = \"Val Acc Média (±std)\"\n",
    "\n",
    "def extract_mean_acc(acc_str):\n",
    "    if isinstance(acc_str, str) and \"±\" in acc_str:\n",
    "        return float(acc_str.split(' ± ')[0])\n",
    "    return -1.0\n",
    "\n",
    "summary_df['MeanAcc'] = summary_df[acc_column_name].apply(extract_mean_acc)\n",
    "\n",
    "if summary_df['MeanAcc'].max() == -1.0:\n",
    "    print(\"ERRO: Não foi possível determinar o melhor modelo. Nenhuma acurácia média válida encontrada.\")\n",
    "    best_model_name = \"Nenhum (Erro na Análise)\"\n",
    "    can_evaluate_best_model = False\n",
    "else:\n",
    "    best_model_name = summary_df.loc[summary_df['MeanAcc'].idxmax()][\"Técnica\"]\n",
    "    print(f\"Melhor modelo com base na acurácia média de validação: {best_model_name}\")\n",
    "    can_evaluate_best_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94cec4",
   "metadata": {},
   "source": [
    "### 10.1 Avaliação do Modelo Baseline no Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8071511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRe-treinando e avaliando Baseline no Conjunto de Teste...\")\n",
    "final_seed_base = NUM_RUNS * 10\n",
    "tf.random.set_seed(final_seed_base)\n",
    "np.random.seed(final_seed_base)\n",
    "\n",
    "model_baseline_final = create_model_baseline()\n",
    "\n",
    "optimizer_base_final = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model_baseline_final.compile(optimizer=optimizer_base_final,\n",
    "                             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "baseline_epochs_avg = EPOCHS\n",
    "if 'Baseline' in results and results['Baseline']:\n",
    "    valid_epochs = [r['best_epoch'] for r in results['Baseline'] if r['best_epoch'] > 0]\n",
    "    if valid_epochs:\n",
    "        baseline_epochs_avg = int(round(np.mean(valid_epochs)))\n",
    "    else:\n",
    "        print(\"Aviso: Baseline não teve 'melhor época' válida nas runs. Usando default de épocas.\")\n",
    "\n",
    "baseline_epochs_avg = max(1, baseline_epochs_avg)\n",
    "\n",
    "print(f\"Treinando baseline por {baseline_epochs_avg} épocas no conjunto completo de treino+validação...\")\n",
    "\n",
    "history_baseline_final = model_baseline_final.fit(x_train_full, y_train_full,\n",
    "                                                epochs=baseline_epochs_avg,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                verbose=0)\n",
    "\n",
    "print(\"Avaliando Baseline no conjunto de teste...\")\n",
    "test_loss_base, test_acc_base = model_baseline_final.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"-> Acurácia Final do Baseline no Teste: {test_acc_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e553ab6",
   "metadata": {},
   "source": [
    "### 10.2 Avaliação do Melhor Modelo no Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if can_evaluate_best_model and best_model_name in model_configs:\n",
    "    print(f\"\\nRe-treinando e avaliando Melhor Modelo ({best_model_name}) no Conjunto de Teste...\")\n",
    "    final_seed_best = NUM_RUNS * 10 + 1\n",
    "    tf.random.set_seed(final_seed_best)\n",
    "    np.random.seed(final_seed_best)\n",
    "\n",
    "    best_model_creator = model_configs[best_model_name]\n",
    "    model_best_final = best_model_creator()\n",
    "\n",
    "    optimizer_best_final = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    model_best_final.compile(optimizer=optimizer_best_final,\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    best_model_epochs_avg = EPOCHS\n",
    "    if best_model_name in results and results[best_model_name]:\n",
    "        valid_epochs_best = [r['best_epoch'] for r in results[best_model_name] if r['best_epoch'] > 0]\n",
    "        if valid_epochs_best:\n",
    "            best_model_epochs_avg = int(round(np.mean(valid_epochs_best)))\n",
    "        else:\n",
    "            print(f\"Aviso: {best_model_name} não teve 'melhor época' válida nas runs. Usando default de épocas.\")\n",
    "\n",
    "    best_model_epochs_avg = max(1, best_model_epochs_avg)\n",
    "\n",
    "    print(f\"Treinando {best_model_name} por {best_model_epochs_avg} épocas no conjunto completo de treino+validação...\")\n",
    "\n",
    "    final_callbacks = []\n",
    "\n",
    "    if best_model_name == \"DataAugmentation\":\n",
    "        datagen_final = ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1\n",
    "        )\n",
    "        print(f\"Usando Data Augmentation para o treino final de {best_model_name}.\")\n",
    "        history_best_final = model_best_final.fit(datagen_final.flow(x_train_full, y_train_full, batch_size=BATCH_SIZE),\n",
    "                                                    steps_per_epoch=len(x_train_full) // BATCH_SIZE,\n",
    "                                                    epochs=best_model_epochs_avg,\n",
    "                                                    verbose=0,\n",
    "                                                    callbacks=final_callbacks)\n",
    "    else:\n",
    "        history_best_final = model_best_final.fit(x_train_full, y_train_full,\n",
    "                                                    epochs=best_model_epochs_avg,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    verbose=0,\n",
    "                                                    callbacks=final_callbacks)\n",
    "\n",
    "    print(f\"Avaliando {best_model_name} no conjunto de teste...\")\n",
    "    test_loss_best, test_acc_best = model_best_final.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"-> Acurácia Final do {best_model_name} no Teste: {test_acc_best:.4f}\")\n",
    "\n",
    "elif not can_evaluate_best_model:\n",
    "    print(\"\\nAvaliação do melhor modelo pulada devido a erro na determinação do melhor modelo.\")\n",
    "else:\n",
    "    print(f\"\\nAvaliação do melhor modelo ({best_model_name}) pulada (modelo não encontrado nas configurações?).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
